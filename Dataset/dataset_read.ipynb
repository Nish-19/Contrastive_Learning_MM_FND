{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 12:59:31.024980: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-31 12:59:31.025316: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import re\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras import models, Model\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "from deep_translator import GoogleTranslator\n",
    "import langid\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/phd/rina_1921cs13/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def get_image(imagepath):\n",
    "\timg = Image.open(imagepath)\n",
    "\timg = img.resize((224,224))\n",
    "\timg = img_to_array(img)\n",
    "\tif img.shape[2]==1:\n",
    "\t\timg = np.stack([img,img,img],axis=2)\n",
    "\t\timg = img.reshape(img.shape[0],img.shape[1],3)\n",
    "\treturn img\n",
    "\n",
    "def cleaned_text(x):\n",
    "\tx = str(x)\n",
    "\tval =  [re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() ]  # if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words\n",
    "\tres = \"\"\n",
    "\tfor i in range(min(300,len(val))):\n",
    "\t\tres = res + val[i] + ' '\n",
    "\tres = res.split()\n",
    "\tres = ' '.join(res)\n",
    "\treturn res\n",
    "\n",
    "def clean_para(text):\n",
    "\tsent_text = nltk.sent_tokenize(text)\n",
    "\tcleaned_sentences = []\n",
    "\tfor i in range(min(len(sent_text), 15)):\n",
    "\t\tcleaned_sentences.append(cleaned_text(sent_text[i]))\n",
    "\tcleaned_para = '. '.join(cleaned_sentences)\n",
    "\treturn cleaned_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "def make_pairs():\n",
    "    # initialize two empty lists to hold the (image, image) pairs and\n",
    "    # labels to indicate if a pair is positive or negative\n",
    "    pairImages = []\n",
    "    pairTexts = []\n",
    "    pairLabels = []\n",
    "    ID = []\n",
    "    print('[INFO] Loading and Processing Dataset...')\n",
    "    source = []\n",
    "    temp = pd.read_csv('/Source/Snopes.csv')\n",
    "    source.append(temp)\n",
    "    temp = pd.read_csv('/Source/Reuters.csv')\n",
    "    source.append(temp)\n",
    "    temp = pd.read_csv('/Source/ReCovery.csv')\n",
    "    source.append(temp)\n",
    "    temp = pd.read_csv('/Source/TICNN.csv')\n",
    "    source.append(temp)\n",
    "    source = pd.concat(source, ignore_index=True, sort=False)\n",
    "    target = []\n",
    "    temp = pd.read_csv('/Target/Snopes.csv')\n",
    "    target.append(temp)\n",
    "    temp = pd.read_csv('/Target/Reuters.csv')\n",
    "    target.append(temp)\n",
    "    temp = pd.read_csv('/Target/ReCovery.csv')\n",
    "    target.append(temp)\n",
    "    temp = pd.read_csv('/Target/TICNN.csv')\n",
    "    target.append(temp)\n",
    "    target = pd.concat(target, ignore_index=True, sort=False)\n",
    "    print('Source columns', source.columns)\n",
    "    print('Target columns', target.columns)\n",
    "    source = source.iloc[:,:].values\n",
    "    target = target.iloc[:,:].values\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading and Processing Dataset...\n",
      "Source columns Index(['Unnamed: 0', 'ID', 'numSources', 'Source_url1', 'Source_text1',\n",
      "       'Image_url1', 'Source_reliability1', 'Source_url2', 'Source_text2',\n",
      "       'Image_url2', 'Source_reliability2', 'Source_url3', 'Source_text3',\n",
      "       'Image_url3', 'Source_reliability3', 'Source_url4', 'Source_text4',\n",
      "       'Image_url4', 'Source_reliability4'],\n",
      "      dtype='object')\n",
      "Target columns Index(['Unnamed: 0', 'ID', 'Target_url', 'Target_text', 'Image_url', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "source, target = make_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source type <class 'numpy.ndarray'>\n",
      "Source shape (6272, 19)\n",
      "Target type <class 'numpy.ndarray'>\n",
      "Target Shape (6272, 6)\n"
     ]
    }
   ],
   "source": [
    "print('Source type', type(source))\n",
    "print('Source shape', source.shape)\n",
    "print('Target type', type(target))\n",
    "print('Target Shape', target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "def get_data(source, target):\n",
    "    pairImages = []\n",
    "    pairTexts = []\n",
    "    pairLabels = []\n",
    "    ID = []\n",
    "    identity = []\n",
    "    ctr = 0\n",
    "    for i in range(len(source)):\n",
    "        if i%100==0:\n",
    "            print('Loading Data...',i)\n",
    "        target_img = ''\n",
    "        try:\n",
    "            target_img = get_image('/TargetImages/' + source[i][1] + '.jpg')\n",
    "        except:\n",
    "            continue\n",
    "        target_txt = cleaned_text(target[i][3])\n",
    "        dataset = ''\n",
    "        if source[i][1][0:3]=='Snp':\n",
    "            dataset = 'Snopes'\n",
    "        elif source[i][1][0:3]=='Rtr':\n",
    "            dataset = 'Reuters'\n",
    "        elif source[i][1][0:3]=='Rcv':\n",
    "            dataset = 'ReCovery'\n",
    "        else:\n",
    "            dataset = 'TICNN'\n",
    "        for j in range(3,3+4*source[i][2],4):\n",
    "            try:\n",
    "                source[i][j+2] = literal_eval(source[i][j+2])\n",
    "            except:\n",
    "                pass\n",
    "            src_txt = clean_para(source[i][j+1])\n",
    "            # src_txt = source[i][j+1]\n",
    "            try:\n",
    "                src_img = get_image('/SourceImages/' + dataset + '/' + source[i][j+2]['image_name'])\n",
    "                pairImages.append([target_img, src_img])\n",
    "                pairTexts.append([target_txt, src_txt])\t\n",
    "                ID.append([source[i][j+2]['image_name']])\n",
    "                identity.append(ctr)\t\t\t\n",
    "                if target[i][5]=='FAKE':\n",
    "                    pairLabels.append([0])\n",
    "                else:\n",
    "                    pairLabels.append([1])\n",
    "            except:\n",
    "                continue\n",
    "        ctr +=1 \n",
    "    # return a 2-tuple of our pairs and labels\n",
    "    return (np.array(pairImages), np.array(pairTexts), np.array(pairLabels), np.array(ID), np.expand_dims(np.array(identity), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data... 0\n",
      "Loading Data... 100\n",
      "Loading Data... 200\n",
      "Loading Data... 300\n",
      "Loading Data... 400\n",
      "Loading Data... 500\n",
      "Loading Data... 600\n",
      "Loading Data... 700\n",
      "Loading Data... 800\n",
      "Loading Data... 900\n",
      "Loading Data... 1000\n",
      "Loading Data... 1100\n",
      "Loading Data... 1200\n",
      "Loading Data... 1300\n",
      "Loading Data... 1400\n",
      "Loading Data... 1500\n",
      "Loading Data... 1600\n",
      "Loading Data... 1700\n",
      "Loading Data... 1800\n",
      "Loading Data... 1900\n",
      "Loading Data... 2000\n",
      "Loading Data... 2100\n",
      "Loading Data... 2200\n",
      "Loading Data... 2300\n",
      "Loading Data... 2400\n",
      "Loading Data... 2500\n",
      "Loading Data... 2600\n",
      "Loading Data... 2700\n",
      "Loading Data... 2800\n",
      "Loading Data... 2900\n",
      "Loading Data... 3000\n",
      "Loading Data... 3100\n",
      "Loading Data... 3200\n",
      "Loading Data... 3300\n",
      "Loading Data... 3400\n",
      "Loading Data... 3500\n",
      "Loading Data... 3600\n",
      "Loading Data... 3700\n",
      "Loading Data... 3800\n",
      "Loading Data... 3900\n",
      "Loading Data... 4000\n",
      "Loading Data... 4100\n",
      "Loading Data... 4200\n",
      "Loading Data... 4300\n",
      "Loading Data... 4400\n",
      "Loading Data... 4500\n",
      "Loading Data... 4600\n",
      "Loading Data... 4700\n",
      "Loading Data... 4800\n",
      "Loading Data... 4900\n",
      "Loading Data... 5000\n",
      "Loading Data... 5100\n",
      "Loading Data... 5200\n",
      "Loading Data... 5300\n",
      "Loading Data... 5400\n",
      "Loading Data... 5500\n",
      "Loading Data... 5600\n",
      "Loading Data... 5700\n",
      "Loading Data... 5800\n",
      "Loading Data... 5900\n",
      "Loading Data... 6000\n",
      "Loading Data... 6100\n",
      "Loading Data... 6200\n"
     ]
    }
   ],
   "source": [
    "img_pair, txt_pair, label, ID, identity = get_data(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11766, 2, 224, 224, 3)\n",
      "(11766, 2)\n",
      "(11766, 1)\n",
      "(11766, 1)\n",
      "(11766, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data analysis\n",
    "print(img_pair.shape)\n",
    "print(txt_pair.shape)\n",
    "print(label.shape) # 0 - Fake, 1 - Real\n",
    "print(ID.shape)\n",
    "print(identity.shape)\n",
    "# Shape: num_samples, 2, features -> 2 is for the source and the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\n",
      " [['Snp1_3_1.jpg']\n",
      " ['Snp3_1_19.jpg']\n",
      " ['Snp3_2_4.jpg']\n",
      " ['Snp3_3_9.jpg']\n",
      " ['Snp3_4_9.jpg']]\n",
      "Identity\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print('ID\\n', ID[:5,:])\n",
    "print('Identity\\n', identity[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the identity\n",
    "np.save('data/identity', identity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a photograph shows four democratic congresswomen posing with a portrait of osama bin laden and an isis flag',\n",
       "       'marion mcgarry a member of the board of representatives for stamford s th district smiles as she chats with guests at the first congregational church s thanksgiving day dinner in stamford conn on thursday nov. marion mcgarry d stamford democratic chairman josh fedeli at left watches as board of representatives for the city of stamford vote on the censure of rep marion mcgarry during a special meeting in the legislative chambers of the government center in stamford conn on tuesday march. a facebook post from the account of marion mcgarry a democrat on the stamford board of representatives. a facebook post from the account of marion mcgarry a democrat on the stamford board of representatives. a facebook post from the account of marion mcgarry a democrat on the stamford board of representatives. a facebook post from the account of marion mcgarry a democrat on the stamford board of representatives. a facebook post from the account of marion mcgarry a democrat on the stamford board of representatives. a facebook post from the account of marion mcgarry a democrat on the stamford board of representatives. a facebook post from the account of marion mcgarry a democrat on the stamford board of representatives. stamford an unapologetic marion mcgarry censured by her colleagues on the board of representatives for posting hate messages on facebook has left the democratic party. mcgarry said she changed her registration to unaffiliated thursday the day after the democratic city committee voted to proceed with a hearing that could have resulted in ousting her from the party. democratic registrar of voters ron malloy confirmed thursday that mcgarry removed her name from the party rolls and re registered as unaffiliated. mcgarry who has represented district for years said she will stay in her seat on the board of representatives for the remaining two years of her term. in her first public statement since the airing of her facebook posts which disparage immigrants muslims and even democrats mcgarry said she has not been afforded due process the fair treatment to which citizens are entitled. i have been stalked abused lied about victimized and denigrated tried and unfairly convicted by the board of representatives the democratic party as well as the media without due process mcgarry wrote'],\n",
       "      dtype='<U14179')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_pair[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fifteen homeless people in chicago were found dead on the street because of record low temperatures that hit the city in late january'\n",
      " 'quindici senzatetto a chicago sarebbero stati trovati morti per strada a causa delle temperature record che hanno colpito la citt alla fine di gennaio. quando la devastante tempesta invernale ha attraversato chicago a fine gennaio gli utenti dei social media forse preoccupati sono caduti in errore con un meme che ha gonfiato il bilancio della tempesta sulla citt. il post che ha avuto origine su twitter il gennaio recitava senzatetto di chicago sono morti congelati. prenditi un momento e pensa a quanto sei fortunato in questo momento. dimentica il muro di trump le mosse della pelosi il jordan che vuoi il nuovo borsellino che vuoi. per un momento pensa a quanto sei fortunato e benedetto. quanto siamo benedetti. mentre il tweet stato visualizzato pi di volte con pi di likes la foto di accompagnamento raffigurante un senzatetto che dorme su una strada innevata stata effettivamente scattata il gennaio e pubblicata nel quotidiano canadese national post. anche sospetto che persone siano state trovate morte congelate per strada. snopes ha contattato la polizia di chicago alla ricerca di ulteriori informazioni ma al momento della pubblicazione dell articolo solo sette morti in vari stati erano state collegate al freddo estremo nel midwest. secondo il sito snopes nessuna delle vittime riportate stata elencata come senzatetto sebbene tre persone siano state trovate morte all aperto in michigan e iowa. l unica fatalit riportata a chicago durante la tempesta riguarda un uomo non identificato colpito da uno spazzaneve. le temperature record a chicago che hanno raggiunto c con un vento freddo di c hanno causato la chiusura di molte scuole e altre strutture e servizi in tutta la citt. in risposta i funzionari della citt hanno messo a disposizione numerose stazioni di polizia biblioteche centri religiosi e altre strutture come centri di accoglienza per chiunque cerchi riparo dal freddo. diversi centri anziani locali hanno anche esteso le ore di apertura e l app di condivisione del traffico lyft ha offerto fino a per coprire i costi dei taxi per le corse nei centri di accoglienza con l uso del codice chijayden']\n",
      "In here 0\n",
      "2129\n",
      "In here 120\n",
      "1858\n",
      "In here 363\n",
      "528\n",
      "In here 801\n",
      "662\n",
      "In here 823\n",
      "1565\n",
      "In here 872\n",
      "1650\n",
      "In here 878\n",
      "1512\n",
      "In here 1216\n",
      "1379\n",
      "In here 1310\n",
      "1985\n",
      "In here 1360\n",
      "1399\n",
      "In here 1361\n",
      "1479\n",
      "In here 1362\n",
      "299\n",
      "In here 1529\n",
      "147\n",
      "In here 1834\n",
      "2699\n",
      "In here 2006\n",
      "2253\n",
      "In here 2065\n",
      "3452\n",
      "In here 2086\n",
      "3261\n",
      "In here 2146\n",
      "1675\n",
      "In here 2155\n",
      "2562\n",
      "In here 2167\n",
      "1957\n",
      "In here 2254\n",
      "119\n",
      "In here 2356\n",
      "514\n",
      "In here 2397\n",
      "1896\n",
      "In here 2503\n",
      "1501\n",
      "In here 2527\n",
      "4999\n",
      "In here 2528\n",
      "1743\n",
      "In here 2615\n",
      "2053\n",
      "In here 2754\n",
      "2119\n",
      "In here 2811\n",
      "1264\n",
      "In here 2812\n",
      "124\n",
      "In here 2816\n",
      "2420\n",
      "In here 2828\n",
      "4855\n",
      "In here 3088\n",
      "2017\n",
      "In here 3273\n",
      "2010\n",
      "In here 3412\n",
      "127\n",
      "In here 3627\n",
      "2566\n",
      "In here 3628\n",
      "2598\n",
      "In here 3766\n",
      "2173\n",
      "In here 3863\n",
      "2209\n",
      "In here 3958\n",
      "51\n",
      "In here 3959\n",
      "1221\n",
      "In here 4000\n",
      "79\n",
      "In here 4155\n",
      "804\n",
      "In here 4188\n",
      "2379\n",
      "In here 4592\n",
      "1957\n",
      "In here 4593\n",
      "3249\n",
      "In here 4615\n",
      "3039\n",
      "In here 4616\n",
      "2305\n",
      "In here 4617\n",
      "3202\n",
      "In here 4618\n",
      "2287\n",
      "In here 4632\n",
      "2239\n",
      "In here 4794\n",
      "2225\n",
      "In here 4820\n",
      "1570\n",
      "In here 4878\n",
      "3112\n",
      "In here 4985\n",
      "1057\n",
      "In here 5005\n",
      "1569\n",
      "In here 5040\n",
      "2797\n",
      "In here 5041\n",
      "2797\n",
      "In here 5071\n",
      "2319\n",
      "In here 5072\n",
      "2345\n",
      "In here 5145\n",
      "2029\n",
      "In here 5285\n",
      "2268\n",
      "In here 5294\n",
      "2484\n",
      "In here 5313\n",
      "2535\n",
      "In here 5375\n",
      "2162\n",
      "In here 5472\n",
      "2109\n",
      "In here 5556\n",
      "1586\n",
      "In here 5630\n",
      "1783\n",
      "In here 5647\n",
      "2788\n",
      "In here 5660\n",
      "2686\n",
      "In here 5777\n",
      "2775\n",
      "In here 5814\n",
      "2654\n",
      "In here 6063\n",
      "1901\n",
      "In here 6114\n",
      "1754\n",
      "In here 6116\n",
      "1682\n",
      "In here 6557\n",
      "1299\n",
      "In here 6586\n",
      "1947\n",
      "In here 6601\n",
      "1289\n",
      "In here 6602\n",
      "2557\n",
      "In here 6904\n",
      "2146\n",
      "In here 6960\n",
      "1145\n",
      "In here 7066\n",
      "2599\n",
      "In here 7076\n",
      "2115\n",
      "In here 7330\n",
      "3003\n",
      "In here 7373\n",
      "1548\n",
      "In here 7556\n",
      "1476\n",
      "In here 7564\n",
      "2052\n",
      "In here 7862\n",
      "2803\n",
      "In here 7926\n",
      "4999\n",
      "In here 7927\n",
      "4999\n",
      "In here 7986\n",
      "2545\n",
      "In here 8276\n",
      "3256\n",
      "In here 8343\n",
      "4999\n",
      "In here 8423\n",
      "4999\n",
      "In here 8447\n",
      "3577\n",
      "In here 9055\n",
      "2105\n",
      "In here 9144\n",
      "1248\n",
      "In here 9236\n",
      "1632\n",
      "In here 9238\n",
      "2010\n",
      "In here 9239\n",
      "1691\n",
      "In here 9240\n",
      "1274\n",
      "In here 9241\n",
      "1673\n",
      "In here 9242\n",
      "2379\n",
      "In here 9243\n",
      "1972\n",
      "In here 9244\n",
      "2027\n",
      "In here 9245\n",
      "2297\n",
      "In here 9246\n",
      "1050\n",
      "In here 9247\n",
      "1050\n",
      "In here 9248\n",
      "2029\n",
      "In here 9249\n",
      "1651\n",
      "In here 9250\n",
      "1737\n",
      "In here 9252\n",
      "2267\n",
      "In here 9253\n",
      "2639\n",
      "In here 9254\n",
      "2639\n",
      "In here 9258\n",
      "2735\n",
      "In here 9259\n",
      "2639\n",
      "In here 9260\n",
      "2135\n",
      "In here 9261\n",
      "1658\n",
      "In here 9262\n",
      "2778\n",
      "In here 9263\n",
      "1975\n",
      "In here 9264\n",
      "3294\n",
      "In here 9265\n",
      "2430\n",
      "In here 9266\n",
      "1837\n",
      "In here 9267\n",
      "3294\n",
      "In here 9268\n",
      "1058\n",
      "In here 9269\n",
      "1226\n",
      "In here 9270\n",
      "2285\n",
      "In here 9271\n",
      "3294\n",
      "In here 9272\n",
      "1058\n",
      "In here 9312\n",
      "2161\n",
      "In here 9321\n",
      "1525\n",
      "In here 9366\n",
      "2459\n",
      "In here 9450\n",
      "965\n",
      "In here 9474\n",
      "638\n",
      "In here 9476\n",
      "54\n",
      "In here 9490\n",
      "665\n",
      "In here 9491\n",
      "207\n",
      "In here 9653\n",
      "3575\n",
      "In here 9654\n",
      "2632\n",
      "In here 9657\n",
      "3575\n",
      "In here 9680\n",
      "21\n",
      "In here 9721\n",
      "1754\n",
      "In here 9753\n",
      "2507\n",
      "In here 9757\n",
      "2630\n",
      "In here 10095\n",
      "1734\n",
      "In here 10107\n",
      "2347\n",
      "In here 10233\n",
      "2368\n",
      "In here 10242\n",
      "1901\n",
      "In here 10245\n",
      "1192\n",
      "In here 10246\n",
      "1721\n",
      "In here 10247\n",
      "2049\n",
      "In here 10275\n",
      "173\n",
      "In here 10277\n",
      "141\n",
      "In here 10295\n",
      "36\n",
      "In here 10314\n",
      "2664\n",
      "In here 10339\n",
      "1815\n",
      "In here 10340\n",
      "2316\n",
      "In here 10344\n",
      "85\n",
      "In here 10360\n",
      "1815\n",
      "In here 10413\n",
      "1940\n",
      "In here 10432\n",
      "1769\n",
      "In here 10481\n",
      "2020\n",
      "In here 10482\n",
      "2317\n",
      "In here 10522\n",
      "690\n",
      "In here 10523\n",
      "1290\n",
      "In here 10528\n",
      "1387\n",
      "In here 10572\n",
      "347\n",
      "In here 10573\n",
      "1501\n",
      "In here 10703\n",
      "17\n",
      "In here 10704\n",
      "318\n",
      "In here 10990\n",
      "2541\n",
      "In here 10999\n",
      "2528\n",
      "In here 11007\n",
      "2393\n",
      "In here 11029\n",
      "2672\n",
      "In here 11144\n",
      "2246\n",
      "In here 11145\n",
      "2363\n",
      "In here 11345\n",
      "1539\n",
      "In here 11459\n",
      "2077\n",
      "In here 11465\n",
      "1932\n",
      "In here 11466\n",
      "1932\n",
      "In here 11467\n",
      "1932\n",
      "In here 11468\n",
      "1932\n",
      "In here 11469\n",
      "1932\n",
      "In here 11630\n",
      "2048\n",
      "In here 11764\n",
      "1342\n",
      "In here 11765\n",
      "1378\n"
     ]
    }
   ],
   "source": [
    "# Text analysis\n",
    "import copy\n",
    "print(txt_pair[0]) # source sentence is not in English\n",
    "#TODO: Force translate all text sentences to English\n",
    "translated_txt_pairs = []\n",
    "for i in range(len(txt_pair)):\n",
    "    lang_target = langid.classify(txt_pair[i][1])[0]\n",
    "    if lang_target != 'en':\n",
    "        print('In here', i)\n",
    "        to_translate = txt_pair[i][1]\n",
    "        if len(to_translate) >= 5000:\n",
    "            to_translate = to_translate[:4999]\n",
    "        print(len(to_translate))\n",
    "        trans_source = GoogleTranslator(source='auto', target='english').translate(to_translate)\n",
    "    else:\n",
    "         trans_source = txt_pair[i][1]\n",
    "    translated_txt_pairs.append([txt_pair[i][0], trans_source])\n",
    "trans_txt_pair = np.array(translated_txt_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "lang_target = langid.classify(txt_pair[0][0])[0]\n",
    "print(lang_target != 'en')\n",
    "print(lang_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11766, 2, 224, 224, 3)\n",
      "(11766, 2)\n",
      "(11766, 1)\n"
     ]
    }
   ],
   "source": [
    "print(img_pair.shape)\n",
    "print(trans_txt_pair.shape)\n",
    "print(label.shape) # 0 - Fake, 1 - Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/image_array', img_pair)\n",
    "np.save('data/text_array_new', trans_txt_pair)\n",
    "np.save('data/labels', label)\n",
    "np.save('data/ids', ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array_load = np.load('data/image_array.npy')\n",
    "txt_array_load = np.load('data/text_array_new.npy')\n",
    "labels_load = np.load('data/labels.npy')\n",
    "ids_load = np.load('data/ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11766, 2)\n"
     ]
    }
   ],
   "source": [
    "txt_array_load = np.load('data/text_array_new.npy')\n",
    "print(txt_array_load.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a photograph shows medalist jim thorpe wearing mismatched shoes at the olympics',\n",
       "       'james francis thorpe sac and fox sauk wa tho huk translated as bright path may or march was an american athlete and olympic gold medalist. a member of the sac and fox nation thorpe became the first native american to win a gold medal for the united states. considered one of the most versatile athletes of modern sports he won two olympic gold medals in the summer olympics one in classic pentathlon and the other in decathlon and played american football collegiate and professional professional baseball and basketball. he lost his olympic titles after it was found he had been paid for playing two seasons of semi professional baseball before competing in the olympics thus violating the amateurism rules that were then in place. in years after his death the international olympic committee ioc restored his olympic medals with replicas after ruling that the decision to strip him of his medals fell outside of the required days but he is to date listed as co champion in both the decathlon and pentathlon events according to official ioc records. thorpe grew up in the sac and fox nation in oklahoma and attended carlisle indian industrial school in carlisle pennsylvania where he was a two time all american for the school s football team under coach pop warner. after his olympic success in which included a record score in the decathlon he added a victory in the all around championship of the amateur athletic union. in thorpe signed with the new york giants and he played six seasons in major league baseball between and. thorpe joined the canton bulldogs american football team in helping them win three professional championships he later played for six teams in the national football league nfl. he played as part of several all american indian teams throughout his career and barnstormed as a professional basketball player with a team composed entirely of american indians. from to thorpe was nominally the first president of the american professional football association apfa which became the nfl in. he played professional sports until age the end of his sports career coinciding with the start of the great depression. he struggled to earn a living after that working several odd jobs. he suffered from alcoholism and lived his last years in failing health and poverty. he was married three times and had eight children before suffering from heart failure and dying in'],\n",
       "      dtype='<U14179')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_array_load[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_array_load.shape)\n",
    "print(txt_array_load.shape)\n",
    "print(labels_load.shape) # 0 - Fake, 1 - Real\n",
    "print(ids_load.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab67cd27b29201e8c839c48b6d9edfe8ecd65f26a69b443cf8553680e00be36"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('nischal': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
