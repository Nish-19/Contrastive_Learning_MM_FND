{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importing Pytorch and initializing the device\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "# Force CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print('Using device', device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# More imports\n",
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import *\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "# from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutput # Code doesn't have this\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers # Can import only this\n",
    "# check this - https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/roi_heads/fast_rcnn.py\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from transformers import BertTokenizer, VisualBertForPreTraining, VisualBertModel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the data\n",
    "img_data = np.load(\"../data/image_array.npy\")\n",
    "txt_data = np.load(\"../data/text_array.npy\")\n",
    "labels_data = np.load(\"../data/labels.npy\")\n",
    "ids_data = np.load(\"../data/ids.npy\")\n",
    "# Printing the shapes\n",
    "print(img_data.shape)\n",
    "print(txt_data.shape)\n",
    "print(labels_data.shape)\n",
    "print(ids_data.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reshape image to -> num_images, sources, num_channels, width, heigth\n",
    "#NOTE: Can convert image data to tensor only in training loop with very less batch size\n",
    "num_images, sources, width, height, num_channels = img_data.shape\n",
    "img_data_reshape = np.reshape(img_data, newshape=(num_images, sources, width, height, num_channels))\n",
    "img_data_target = img_data_reshape[:,0,:,:,:] # Don't convert to GPU\n",
    "img_data_source = img_data_reshape[:,1,:,:,:] # Don't convert to GPU\n",
    "print('New Target Shape', img_data_target.shape)\n",
    "print('New Source Shape', img_data_source.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: Create rgb cv2 batch\n",
    "img_cv2_source = []\n",
    "img_cv2_target = []\n",
    "for i in range(len(img_data_source)):\n",
    "    img_cv2_source.append(cv2.cvtColor(img_data_source[i], cv2.COLOR_RGB2BGR))\n",
    "    img_cv2_target.append(cv2.cvtColor(img_data_target[i], cv2.COLOR_RGB2BGR))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loading Config and Model Weights\n",
    "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "def load_config_and_model_weights(cfg_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
    "\n",
    "    # ROI HEADS SCORE THRESHOLD\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "    # Comment the next line if you're using 'cuda'\n",
    "    # cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
    "\n",
    "    return cfg\n",
    "cfg = load_config_and_model_weights(cfg_path) \n",
    "\n",
    "def get_model(cfg):\n",
    "    # build model\n",
    "    model = build_model(cfg)\n",
    "\n",
    "    # load weights\n",
    "    checkpointer = DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = get_model(cfg).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert Image to Model input\n",
    "def prepare_image_inputs(cfg, img_list):\n",
    "    # Resizing the image according to the configuration\n",
    "    transform_gen = T.ResizeShortestEdge(\n",
    "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "            )\n",
    "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "    # Convert to C,H,W format\n",
    "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "    # Normalizing the image\n",
    "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "    # Convert to ImageList\n",
    "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
    "    \n",
    "    return images, batched_inputs\n",
    "\n",
    "# source_images, source_batched_inputs = prepare_image_inputs(cfg, img_cv2_source[:32]) # Need to use very less batch size\n",
    "# # target_images, target_batched_inputs = prepare_image_inputs(cfg, img_cv2_target[:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Getting features\n",
    "def get_features(model, images):\n",
    "    model = model.to(device)\n",
    "    features = model.backbone((images.tensor).to(device))\n",
    "    return features\n",
    "\n",
    "# source_features = get_features(model, source_images)\n",
    "# # target_features = get_features(model, target_images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Getting region proposals \n",
    "def get_proposals(model, images, features):\n",
    "    proposals, _ = model.proposal_generator(images, features)\n",
    "    return proposals\n",
    "\n",
    "# source_proposals = get_proposals(model, source_images, source_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get box features for proposal\n",
    "def get_box_features(model, features, proposals):\n",
    "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
    "\n",
    "    box_features = box_features.reshape(features['p2'].shape[0], 1000, 1024) # depends on your config and batch size\n",
    "    return box_features, features_list\n",
    "\n",
    "# source_box_features, source_features_list = get_box_features(model, source_features, source_proposals)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get prediction logits and boxes\n",
    "def get_prediction_logits(model, features_list, proposals):\n",
    "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    cls_features = model.roi_heads.box_head(cls_features)\n",
    "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
    "    return pred_class_logits, pred_proposal_deltas\n",
    "\n",
    "# pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, source_features_list, source_proposals)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get FastRCNN scores and batches - Base Code\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import torch\n",
    "from fvcore.nn import giou_loss, smooth_l1_loss\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.config import configurable\n",
    "from detectron2.layers import ShapeSpec, batched_nms, cat, cross_entropy, nonzero_tuple\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.utils.events import get_event_storage\n",
    "\n",
    "__all__ = [\"fast_rcnn_inference\", \"FastRCNNOutputLayers\"]\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "Shape shorthand in this module:\n",
    "\n",
    "    N: number of images in the minibatch\n",
    "    R: number of ROIs, combined over all images, in the minibatch\n",
    "    Ri: number of ROIs in image i\n",
    "    K: number of foreground classes. E.g.,there are 80 foreground classes in COCO.\n",
    "\n",
    "Naming convention:\n",
    "\n",
    "    deltas: refers to the 4-d (dx, dy, dw, dh) deltas that parameterize the box2box\n",
    "    transform (see :class:`box_regression.Box2BoxTransform`).\n",
    "\n",
    "    pred_class_logits: predicted class scores in [-inf, +inf]; use\n",
    "        softmax(pred_class_logits) to estimate P(class).\n",
    "\n",
    "    gt_classes: ground-truth classification labels in [0, K], where [0, K) represent\n",
    "        foreground object classes and K represents the background class.\n",
    "\n",
    "    pred_proposal_deltas: predicted box2box transform deltas for transforming proposals\n",
    "        to detection box predictions.\n",
    "\n",
    "    gt_proposal_deltas: ground-truth box2box transform deltas\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fast_rcnn_inference(\n",
    "    boxes: List[torch.Tensor],\n",
    "    scores: List[torch.Tensor],\n",
    "    image_shapes: List[Tuple[int, int]],\n",
    "    score_thresh: float,\n",
    "    nms_thresh: float,\n",
    "    topk_per_image: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Call `fast_rcnn_inference_single_image` for all images.\n",
    "\n",
    "    Args:\n",
    "        boxes (list[Tensor]): A list of Tensors of predicted class-specific or class-agnostic\n",
    "            boxes for each image. Element i has shape (Ri, K * 4) if doing\n",
    "            class-specific regression, or (Ri, 4) if doing class-agnostic\n",
    "            regression, where Ri is the number of predicted objects for image i.\n",
    "            This is compatible with the output of :meth:`FastRCNNOutputLayers.predict_boxes`.\n",
    "        scores (list[Tensor]): A list of Tensors of predicted class scores for each image.\n",
    "            Element i has shape (Ri, K + 1), where Ri is the number of predicted objects\n",
    "            for image i. Compatible with the output of :meth:`FastRCNNOutputLayers.predict_probs`.\n",
    "        image_shapes (list[tuple]): A list of (width, height) tuples for each image in the batch.\n",
    "        score_thresh (float): Only return detections with a confidence score exceeding this\n",
    "            threshold.\n",
    "        nms_thresh (float):  The threshold to use for box non-maximum suppression. Value in [0, 1].\n",
    "        topk_per_image (int): The number of top scoring detections to return. Set < 0 to return\n",
    "            all detections.\n",
    "\n",
    "    Returns:\n",
    "        instances: (list[Instances]): A list of N instances, one for each image in the batch,\n",
    "            that stores the topk most confidence detections.\n",
    "        kept_indices: (list[Tensor]): A list of 1D tensor of length of N, each element indicates\n",
    "            the corresponding boxes/scores index in [0, Ri) from the input, for image i.\n",
    "    \"\"\"\n",
    "    result_per_image = [\n",
    "        fast_rcnn_inference_single_image(\n",
    "            boxes_per_image, scores_per_image, image_shape, score_thresh, nms_thresh, topk_per_image\n",
    "        )\n",
    "        for scores_per_image, boxes_per_image, image_shape in zip(scores, boxes, image_shapes)\n",
    "    ]\n",
    "    return [x[0] for x in result_per_image], [x[1] for x in result_per_image]\n",
    "\n",
    "\n",
    "def _log_classification_stats(pred_logits, gt_classes, prefix=\"fast_rcnn\"):\n",
    "    \"\"\"\n",
    "    Log the classification metrics to EventStorage.\n",
    "\n",
    "    Args:\n",
    "        pred_logits: Rx(K+1) logits. The last column is for background class.\n",
    "        gt_classes: R labels\n",
    "    \"\"\"\n",
    "    num_instances = gt_classes.numel()\n",
    "    if num_instances == 0:\n",
    "        return\n",
    "    pred_classes = pred_logits.argmax(dim=1)\n",
    "    bg_class_ind = pred_logits.shape[1] - 1\n",
    "\n",
    "    fg_inds = (gt_classes >= 0) & (gt_classes < bg_class_ind)\n",
    "    num_fg = fg_inds.nonzero().numel()\n",
    "    fg_gt_classes = gt_classes[fg_inds]\n",
    "    fg_pred_classes = pred_classes[fg_inds]\n",
    "\n",
    "    num_false_negative = (fg_pred_classes == bg_class_ind).nonzero().numel()\n",
    "    num_accurate = (pred_classes == gt_classes).nonzero().numel()\n",
    "    fg_num_accurate = (fg_pred_classes == fg_gt_classes).nonzero().numel()\n",
    "\n",
    "    storage = get_event_storage()\n",
    "    storage.put_scalar(f\"{prefix}/cls_accuracy\", num_accurate / num_instances)\n",
    "    if num_fg > 0:\n",
    "        storage.put_scalar(f\"{prefix}/fg_cls_accuracy\", fg_num_accurate / num_fg)\n",
    "        storage.put_scalar(f\"{prefix}/false_negative\", num_false_negative / num_fg)\n",
    "\n",
    "\n",
    "def fast_rcnn_inference_single_image(\n",
    "    boxes,\n",
    "    scores,\n",
    "    image_shape: Tuple[int, int],\n",
    "    score_thresh: float,\n",
    "    nms_thresh: float,\n",
    "    topk_per_image: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single-image inference. Return bounding-box detection results by thresholding\n",
    "    on scores and applying non-maximum suppression (NMS).\n",
    "\n",
    "    Args:\n",
    "        Same as `fast_rcnn_inference`, but with boxes, scores, and image shapes\n",
    "        per image.\n",
    "\n",
    "    Returns:\n",
    "        Same as `fast_rcnn_inference`, but for only one image.\n",
    "    \"\"\"\n",
    "    valid_mask = torch.isfinite(boxes).all(dim=1) & torch.isfinite(scores).all(dim=1)\n",
    "    if not valid_mask.all():\n",
    "        boxes = boxes[valid_mask]\n",
    "        scores = scores[valid_mask]\n",
    "\n",
    "    scores = scores[:, :-1]\n",
    "    num_bbox_reg_classes = boxes.shape[1] // 4\n",
    "    # Convert to Boxes to use the `clip` function ...\n",
    "    boxes = Boxes(boxes.reshape(-1, 4))\n",
    "    boxes.clip(image_shape)\n",
    "    boxes = boxes.tensor.view(-1, num_bbox_reg_classes, 4)  # R x C x 4\n",
    "\n",
    "    # 1. Filter results based on detection scores. It can make NMS more efficient\n",
    "    #    by filtering out low-confidence detections.\n",
    "    filter_mask = scores > score_thresh  # R x K\n",
    "    # R' x 2. First column contains indices of the R predictions;\n",
    "    # Second column contains indices of classes.\n",
    "    filter_inds = filter_mask.nonzero()\n",
    "    if num_bbox_reg_classes == 1:\n",
    "        boxes = boxes[filter_inds[:, 0], 0]\n",
    "    else:\n",
    "        boxes = boxes[filter_mask]\n",
    "    scores = scores[filter_mask]\n",
    "\n",
    "    # 2. Apply NMS for each class independently.\n",
    "    keep = batched_nms(boxes, scores, filter_inds[:, 1], nms_thresh)\n",
    "    if topk_per_image >= 0:\n",
    "        keep = keep[:topk_per_image]\n",
    "    boxes, scores, filter_inds = boxes[keep], scores[keep], filter_inds[keep]\n",
    "\n",
    "    result = Instances(image_shape)\n",
    "    result.pred_boxes = Boxes(boxes)\n",
    "    result.scores = scores\n",
    "    result.pred_classes = filter_inds[:, 1]\n",
    "    return result, filter_inds[:, 0]\n",
    "\n",
    "\n",
    "class FastRCNNOutputs:\n",
    "    \"\"\"\n",
    "    An internal implementation that stores information about outputs of a Fast R-CNN head,\n",
    "    and provides methods that are used to decode the outputs of a Fast R-CNN head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta=0.0,\n",
    "        box_reg_loss_type=\"smooth_l1\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            box2box_transform (Box2BoxTransform/Box2BoxTransformRotated):\n",
    "                box2box transform instance for proposal-to-detection transformations.\n",
    "            pred_class_logits (Tensor): A tensor of shape (R, K + 1) storing the predicted class\n",
    "                logits for all R predicted object instances.\n",
    "                Each row corresponds to a predicted object instance.\n",
    "            pred_proposal_deltas (Tensor): A tensor of shape (R, K * B) or (R, B) for\n",
    "                class-specific or class-agnostic regression. It stores the predicted deltas that\n",
    "                transform proposals into final box detections.\n",
    "                B is the box dimension (4 or 5).\n",
    "                When B is 4, each row is [dx, dy, dw, dh (, ....)].\n",
    "                When B is 5, each row is [dx, dy, dw, dh, da (, ....)].\n",
    "            proposals (list[Instances]): A list of N Instances, where Instances i stores the\n",
    "                proposals for image i, in the field \"proposal_boxes\".\n",
    "                When training, each Instances must have ground-truth labels\n",
    "                stored in the field \"gt_classes\" and \"gt_boxes\".\n",
    "                The total number of all instances must be equal to R.\n",
    "            smooth_l1_beta (float): The transition point between L1 and L2 loss in\n",
    "                the smooth L1 loss function. When set to 0, the loss becomes L1. When\n",
    "                set to +inf, the loss becomes constant 0.\n",
    "            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\"\n",
    "        \"\"\"\n",
    "        self.box2box_transform = box2box_transform\n",
    "        self.num_preds_per_image = [len(p) for p in proposals]\n",
    "        self.pred_class_logits = pred_class_logits\n",
    "        self.pred_proposal_deltas = pred_proposal_deltas\n",
    "        self.smooth_l1_beta = smooth_l1_beta\n",
    "        self.box_reg_loss_type = box_reg_loss_type\n",
    "\n",
    "        self.image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        if len(proposals):\n",
    "            box_type = type(proposals[0].proposal_boxes)\n",
    "            # cat(..., dim=0) concatenates over all images in the batch\n",
    "            self.proposals = box_type.cat([p.proposal_boxes for p in proposals])\n",
    "            assert (\n",
    "                not self.proposals.tensor.requires_grad\n",
    "            ), \"Proposals should not require gradients!\"\n",
    "\n",
    "            # \"gt_classes\" exists if and only if training. But other gt fields may\n",
    "            # not necessarily exist in training for images that have no groundtruth.\n",
    "            if proposals[0].has(\"gt_classes\"):\n",
    "                self.gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n",
    "\n",
    "                # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
    "                # should not be included in regression loss computation.\n",
    "                # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
    "                # value won't be used in self.box_reg_loss().\n",
    "                gt_boxes = [\n",
    "                    p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes for p in proposals\n",
    "                ]\n",
    "                self.gt_boxes = box_type.cat(gt_boxes)\n",
    "        else:\n",
    "            self.proposals = Boxes(torch.zeros(0, 4, device=self.pred_proposal_deltas.device))\n",
    "        self._no_instances = len(self.proposals) == 0  # no instances found\n",
    "\n",
    "    def softmax_cross_entropy_loss(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        _log_classification_stats(self.pred_class_logits, self.gt_classes)\n",
    "        return cross_entropy(self.pred_class_logits, self.gt_classes, reduction=\"mean\")\n",
    "\n",
    "    def box_reg_loss(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        if self._no_instances:\n",
    "            return 0.0 * self.pred_proposal_deltas.sum()\n",
    "\n",
    "        box_dim = self.proposals.tensor.size(1)  # 4 or 5\n",
    "        cls_agnostic_bbox_reg = self.pred_proposal_deltas.size(1) == box_dim\n",
    "        device = self.pred_proposal_deltas.device\n",
    "\n",
    "        bg_class_ind = self.pred_class_logits.shape[1] - 1\n",
    "        # Box delta loss is only computed between the prediction for the gt class k\n",
    "        # (if 0 <= k < bg_class_ind) and the target; there is no loss defined on predictions\n",
    "        # for non-gt classes and background.\n",
    "        # Empty fg_inds should produce a valid loss of zero because reduction=sum.\n",
    "        fg_inds = nonzero_tuple((self.gt_classes >= 0) & (self.gt_classes < bg_class_ind))[0]\n",
    "\n",
    "        if cls_agnostic_bbox_reg:\n",
    "            # pred_proposal_deltas only corresponds to foreground class for agnostic\n",
    "            gt_class_cols = torch.arange(box_dim, device=device)\n",
    "        else:\n",
    "            # pred_proposal_deltas for class k are located in columns [b * k : b * k + b],\n",
    "            # where b is the dimension of box representation (4 or 5)\n",
    "            # Note that compared to Detectron1,\n",
    "            # we do not perform bounding box regression for background classes.\n",
    "            gt_class_cols = box_dim * self.gt_classes[fg_inds, None] + torch.arange(\n",
    "                box_dim, device=device\n",
    "            )\n",
    "\n",
    "        if self.box_reg_loss_type == \"smooth_l1\":\n",
    "            gt_proposal_deltas = self.box2box_transform.get_deltas(\n",
    "                self.proposals.tensor, self.gt_boxes.tensor\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                self.pred_proposal_deltas[fg_inds[:, None], gt_class_cols],\n",
    "                gt_proposal_deltas[fg_inds],\n",
    "                self.smooth_l1_beta,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        elif self.box_reg_loss_type == \"giou\":\n",
    "            fg_pred_boxes = self.box2box_transform.apply_deltas(\n",
    "                self.pred_proposal_deltas[fg_inds[:, None], gt_class_cols],\n",
    "                self.proposals.tensor[fg_inds],\n",
    "            )\n",
    "            loss_box_reg = giou_loss(\n",
    "                fg_pred_boxes,\n",
    "                self.gt_boxes.tensor[fg_inds],\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\")\n",
    "\n",
    "        loss_box_reg = loss_box_reg / self.gt_classes.numel()\n",
    "        return loss_box_reg\n",
    "\n",
    "    def losses(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        return {\"loss_cls\": self.softmax_cross_entropy_loss(), \"loss_box_reg\": self.box_reg_loss()}\n",
    "\n",
    "    def predict_boxes(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        pred = self.box2box_transform.apply_deltas(self.pred_proposal_deltas, self.proposals.tensor)\n",
    "        return pred.split(self.num_preds_per_image, dim=0)\n",
    "\n",
    "    def predict_probs(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        probs = F.softmax(self.pred_class_logits, dim=-1)\n",
    "        return probs.split(self.num_preds_per_image, dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Getting box scores\n",
    "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas):\n",
    "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "    outputs = FastRCNNOutputs(\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        source_proposals,\n",
    "        smooth_l1_beta,\n",
    "    )\n",
    "\n",
    "    boxes = outputs.predict_boxes()\n",
    "    scores = outputs.predict_probs()\n",
    "    image_shapes = outputs.image_shapes\n",
    "\n",
    "    return boxes, scores, image_shapes\n",
    "\n",
    "# source_boxes, source_scores, source_image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Rescale boxes to the original size\n",
    "def get_output_boxes(boxes, batched_inputs, image_size):\n",
    "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "    output_boxes = Boxes(proposal_boxes)\n",
    "\n",
    "    output_boxes.scale(scale_x, scale_y)\n",
    "    output_boxes.clip(image_size)\n",
    "\n",
    "    return output_boxes\n",
    "\n",
    "# output_boxes = [get_output_boxes(source_boxes[i], source_batched_inputs[i], source_proposals[i].image_size) for i in range(len(source_proposals))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Select boxes using NMS\n",
    "def select_boxes(cfg, output_boxes, scores):\n",
    "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "    cls_prob = scores.detach()\n",
    "    cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
    "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "        cls_scores = cls_prob[:, cls_ind+1]\n",
    "        det_boxes = cls_boxes[:,cls_ind,:]\n",
    "        # print(type(det_boxes))\n",
    "        # print(type(cls_scores))\n",
    "        # print(type(test_nms_thresh))\n",
    "        keep = np.array(nms(det_boxes.cpu(), cls_scores.cpu(), test_nms_thresh))\n",
    "        max_conf[keep] = torch.where(cls_scores.cpu()[keep] > max_conf.cpu()[keep], cls_scores.cpu()[keep], max_conf.cpu()[keep])\n",
    "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "    return keep_boxes, max_conf\n",
    "\n",
    "# temp = [select_boxes(cfg, output_boxes[i], source_scores[i]) for i in range(len(source_scores))]\n",
    "# keep_boxes, max_conf = [],[]\n",
    "# for keep_box, mx_conf in temp:\n",
    "#     keep_boxes.append(keep_box)\n",
    "#     max_conf.append(mx_conf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Limit the number of embeddings\n",
    "MIN_BOXES=10\n",
    "MAX_BOXES=100\n",
    "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "    if len(keep_boxes) < min_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "    elif len(keep_boxes) > max_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "    return keep_boxes\n",
    "\n",
    "# keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Getting the final visual embeddings\n",
    "def get_visual_embeds(box_features, keep_boxes):\n",
    "    return box_features[keep_boxes.copy()]\n",
    "\n",
    "# visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(source_box_features, keep_boxes)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Getting the visual embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# text_batch = txt_data[0:32,0].tolist()\n",
    "# tokens = tokenizer(text_batch, padding='max_length', max_length=100)\n",
    "# input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "# attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "# token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).to(device)\n",
    "# visual_embeds = torch.stack(visual_embeds).to(device)\n",
    "# visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "# visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "# # model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre').to(device) # this checkpoint has 1024 dimensional visual embeddings projection\n",
    "# model = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre').to(device)\n",
    "# outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), token_type_ids=token_type_ids.to(device), visual_embeds=visual_embeds.to(device), visual_attention_mask=visual_attention_mask.to(device), visual_token_type_ids=visual_token_type_ids.to(device))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Need to Pool output in dim=1\n",
    "pool_layer = nn.MaxPool2d((200, 1))\n",
    "# out_pool = pool_layer(outputs['last_hidden_state']).squeeze(1)\n",
    "# print(out_pool.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO: Batch Input Processing for VisualBERT\n",
    "choice = 0 # 0 for source and 1 for target\n",
    "output_array = []\n",
    "for i in range(0, len(img_cv2_source), 32):\n",
    "    print(i)\n",
    "    start_range = i\n",
    "    end_range = i +32\n",
    "    if i+32>= len(img_cv2_source):\n",
    "        end_range = len(img_cv2_source)\n",
    "    if i == 64:\n",
    "        break\n",
    "    source_images, source_batched_inputs = prepare_image_inputs(cfg, img_cv2_source[start_range:end_range])\n",
    "    source_features = get_features(model, source_images)\n",
    "    source_proposals = get_proposals(model, source_images, source_features)\n",
    "    source_box_features, source_features_list = get_box_features(model, source_features, source_proposals)\n",
    "    pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, source_features_list, source_proposals)\n",
    "    source_boxes, source_scores, source_image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)\n",
    "    output_boxes = [get_output_boxes(source_boxes[i], source_batched_inputs[i], source_proposals[i].image_size) for i in range(len(source_proposals))]\n",
    "    temp = [select_boxes(cfg, output_boxes[i], source_scores[i]) for i in range(len(source_scores))]\n",
    "    keep_boxes, max_conf = [],[]\n",
    "    for keep_box, mx_conf in temp:\n",
    "        keep_boxes.append(keep_box)\n",
    "        max_conf.append(mx_conf)\n",
    "    keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
    "    visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(source_box_features, keep_boxes)]\n",
    "    # Brining visual BERT\n",
    "    text_batch = txt_data[start_range:end_range,choice].tolist()\n",
    "    tokens = tokenizer(text_batch, padding='max_length', max_length=100)\n",
    "    input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "    token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).to(device)\n",
    "    visual_embeds = torch.stack(visual_embeds).to(device)\n",
    "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "    # model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre').to(device) # this checkpoint has 1024 dimensional visual embeddings projection\n",
    "    trans_model = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre').to(device)\n",
    "    outputs = trans_model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), token_type_ids=token_type_ids.to(device), visual_embeds=visual_embeds.to(device), visual_attention_mask=visual_attention_mask.to(device), visual_token_type_ids=visual_token_type_ids.to(device))\n",
    "    out_pool = pool_layer(outputs['last_hidden_state']).squeeze(1).cpu().detach().numpy()\n",
    "    output_array.append(out_pool)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_shape = output_array.shape[0]*output_array.shape[1], output_array.shape[2]\n",
    "output_array = np.array(output_array).reshape(new_shape)\n",
    "print(output_array.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.save('../data/source_mm_vbert.npy', output_array)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4396559e1bf5c32d41ffba8c548c213f47855d2d4ac82024dddc5ba3ffea815"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.11 64-bit ('newnisbert': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}