{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_ENVIRONMENT_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from numpy import asarray,zeros\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11766, 768)\n",
      "(11766, 768)\n",
      "(11766,)\n",
      "(11766,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "bert_source = np.load(\"../data/bert_text_only_source.npy\")\n",
    "bert_target = np.load(\"../data/bert_text_only_target.npy\")\n",
    "labels_data = np.load(\"../data/labels.npy\").squeeze(1)\n",
    "ids_data = np.load(\"../data/ids.npy\").squeeze(1)\n",
    "# Printing the shapes\n",
    "print(bert_source.shape)\n",
    "print(bert_target.shape)\n",
    "print(labels_data.shape)\n",
    "print(ids_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(batch_size, target_data, source_data, labels, split_type = 'train'):\n",
    "    target_data = torch.tensor(target_data)\n",
    "    source_data = torch.tensor(source_data)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    data = TensorDataset(target_data, source_data, labels)\n",
    "    if split_type == 'train':\n",
    "        sampler = RandomSampler(data)\n",
    "    elif split_type == 'val':\n",
    "        sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "    return data, sampler, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_target_data, test_target_data, train_source_data, test_source_data, train_labels, test_labels = train_test_split(bert_target, bert_source, labels_data, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(ids_data, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data, train_sampler, train_dataloader = get_data_loader(batch_size, train_target_data, train_source_data, train_labels, 'train')\n",
    "test_data, test_sampler, test_dataloader = get_data_loader(batch_size, test_target_data, test_source_data, test_labels, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Model\n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTModel, self).__init__()\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.linear_1 = nn.Linear(1536, 512, bias=True)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.linear_2 = nn.Linear(512, 128, bias=True)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.classification = nn.Linear(128, 2)\n",
    "    def forward(self, target_features, source_features):\n",
    "        concat_tensor = torch.cat([target_features, source_features], dim=1)\n",
    "        inter_1 = self.tanh1(self.linear_1(concat_tensor))\n",
    "        inter_2 = self.tanh2(self.linear_2(inter_1))\n",
    "        logits = self.classification(inter_2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "model = BERTModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "def get_optimizer_scheduler(name, model, train_dataloader_len, epochs, lr_set):\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                lr = lr_set, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "    )\n",
    "\n",
    "    total_steps = train_dataloader_len * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = total_steps//2, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the optimizer and scheduler\n",
    "epochs = 5\n",
    "lr = 3e-5 # Less LR\n",
    "# lr = 0.5\n",
    "iters_to_accumulate = 2\n",
    "name = \"Adam\"\n",
    "# name = \"LARS-SGD\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer, scheduler = get_optimizer_scheduler(name, model, len(train_dataloader), epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Evaluating Loss ######################\n",
    "#######################################################\n",
    "def evaluate_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for it, (target_inputs, source_inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "            target_inputs, source_inputs, labels = target_inputs.to(device), source_inputs.to(device), labels.to(device)\n",
    "            logits = net(target_inputs, source_inputs)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels).item() # initially it was logits.squeeze(-1)\n",
    "            count += 1\n",
    "    return mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Flat Accuracy Calculation ####################\n",
    "###############################################################\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "################ Validation Accuracy Calculation ####################\n",
    "###############################################################\n",
    "def evaluate_accuracy(model, device, validation_dataloader):\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\t    \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_t_inputs, b_s_inputs, b_labels = batch\t    \n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad(): \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            logits = model(b_t_inputs, b_s_inputs)       \n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    accuracy = eval_accuracy/nb_eval_steps\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
    "    best_loss = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # Iterating over all epochs\n",
    "    for ep in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (target_inputs, source_inputs, labels) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            # Converting to cuda tensors\n",
    "            target_inputs, source_inputs, labels = target_inputs.to(device), source_inputs.to(device), labels.to(device)\n",
    "    \t\t\n",
    "            # Obtaining the logits from the model\n",
    "            logits = net(target_inputs, source_inputs)\n",
    "            # print(logits.device)\n",
    "\n",
    "            # Computing loss\n",
    "            # print(logits.squeeze(-1).shape)\n",
    "            # print(labels.shape)\n",
    "            loss = criterion(logits.squeeze(-1), labels)\n",
    "            loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
    "\n",
    "            # Backpropagating the gradients\n",
    "            # Calls backward()\n",
    "            loss.backward()\n",
    "\n",
    "            if (it + 1) % iters_to_accumulate == 0:\n",
    "                # Optimization step\n",
    "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
    "                # otherwise, opti.step() is skipped.\n",
    "                opti.step()\n",
    "                # Adjust the learning rate based on the number of iterations.\n",
    "                lr_scheduler.step()\n",
    "                # Clear gradients\n",
    "                net.zero_grad()\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        val_accuracy = evaluate_accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Epoch {} complete! Validation Accuracy : {}\".format(ep+1, val_accuracy))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "    # Saving the model\n",
    "    path_to_model='saved_models/lr_{}_val_loss_{}_ep_{}_text_only.pt'.format(lr, round(best_loss, 5), best_ep)\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\n",
    "    net.load_state_dict(torch.load(path_to_model)) # Re-Loading the best model\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 52/148 [00:00<00:00, 174.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 29/148 of epoch 1 complete. Loss : 0.3485366712356436 \n",
      "\n",
      "Iteration 58/148 of epoch 1 complete. Loss : 0.34677284853211765 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 114/148 [00:00<00:00, 195.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 87/148 of epoch 1 complete. Loss : 0.34017938272706394 \n",
      "\n",
      "Iteration 116/148 of epoch 1 complete. Loss : 0.33595694344619226 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 190.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 145/148 of epoch 1 complete. Loss : 0.33229528418902693 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 459.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 complete! Validation Loss : 0.6685686981355822\n",
      "Epoch 1 complete! Validation Accuracy : 0.5626013513513514\n",
      "Best validation loss improved from inf to 0.6685686981355822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 21/148 [00:00<00:00, 207.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 29/148 of epoch 2 complete. Loss : 0.32639308016875695 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 64/148 [00:00<00:00, 211.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 58/148 of epoch 2 complete. Loss : 0.32718196614035244 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 86/148 [00:00<00:00, 212.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 87/148 of epoch 2 complete. Loss : 0.31860919553658057 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 130/148 [00:00<00:00, 214.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 116/148 of epoch 2 complete. Loss : 0.316150039434433 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 212.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 145/148 of epoch 2 complete. Loss : 0.30957569027769155 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 539.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 complete! Validation Loss : 0.6160167520110672\n",
      "Epoch 2 complete! Validation Accuracy : 0.6463851351351352\n",
      "Best validation loss improved from 0.6685686981355822 to 0.6160167520110672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 22/148 [00:00<00:00, 216.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 29/148 of epoch 3 complete. Loss : 0.30176105375947626 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 67/148 [00:00<00:00, 219.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 58/148 of epoch 3 complete. Loss : 0.2956420450374998 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 89/148 [00:00<00:00, 216.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 87/148 of epoch 3 complete. Loss : 0.2882735462024294 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 111/148 [00:00<00:00, 211.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 116/148 of epoch 3 complete. Loss : 0.284658400149181 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 213.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 145/148 of epoch 3 complete. Loss : 0.273715587011699 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 535.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 complete! Validation Loss : 0.5429091582427154\n",
      "Epoch 3 complete! Validation Accuracy : 0.7459797297297297\n",
      "Best validation loss improved from 0.6160167520110672 to 0.5429091582427154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 22/148 [00:00<00:00, 218.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 29/148 of epoch 4 complete. Loss : 0.26840604482025937 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 65/148 [00:00<00:00, 208.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 58/148 of epoch 4 complete. Loss : 0.25910601019859314 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 86/148 [00:00<00:00, 207.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 87/148 of epoch 4 complete. Loss : 0.24787691903525386 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 128/148 [00:00<00:00, 206.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 116/148 of epoch 4 complete. Loss : 0.24377916907442027 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 208.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 145/148 of epoch 4 complete. Loss : 0.2372358842142697 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 512.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 complete! Validation Loss : 0.46383886965545446\n",
      "Epoch 4 complete! Validation Accuracy : 0.8057601351351351\n",
      "Best validation loss improved from 0.5429091582427154 to 0.46383886965545446\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 21/148 [00:00<00:00, 209.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 29/148 of epoch 5 complete. Loss : 0.2175942027363284 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 64/148 [00:00<00:00, 207.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 58/148 of epoch 5 complete. Loss : 0.23491663768373686 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 86/148 [00:00<00:00, 208.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 87/148 of epoch 5 complete. Loss : 0.2148934412619163 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 129/148 [00:00<00:00, 208.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 116/148 of epoch 5 complete. Loss : 0.2106730619381214 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 208.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 145/148 of epoch 5 complete. Loss : 0.20410434747564382 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 525.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 complete! Validation Loss : 0.4100561278897363\n",
      "Epoch 5 complete! Validation Accuracy : 0.8169763513513513\n",
      "Best validation loss improved from 0.46383886965545446 to 0.4100561278897363\n",
      "\n",
      "The model has been saved in saved_models/lr_3e-05_val_loss_0.41006_ep_5_text_only.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(model, criterion, optimizer, lr, scheduler, train_dataloader, test_dataloader, epochs, iters_to_accumulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction_dataloader, model, model_name, path_to_model, load = False):\n",
    "    # Prediction on test set\n",
    "    if load:\n",
    "        print(\"Loading the weights of the model...\")\n",
    "        model.load_state_dict(torch.load(path_to_model))\n",
    "\n",
    "    print('Evaluating on the testset')\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_t_inputs, b_s_inputs, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            logits = model(b_t_inputs, b_s_inputs)\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.extend(pred_flat)\n",
    "        true_labels.extend(labels_flat)\n",
    "    # Code for result display\n",
    "    print(model_name, 'Text Only BERT Classification accuracy is')\n",
    "    print(metrics.accuracy_score(true_labels, predictions)*100)\n",
    "    print(classification_report(true_labels, predictions, target_names = ['fake', 'real']))\n",
    "\t# For error analysis\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['ids'], new_df['ground_truth'], new_df['predicted'] = test_ids, true_labels, predictions\n",
    "    new_df.to_csv(str('error_analysis/'+model_name+'.csv'), index=False)  \n",
    "\t# Converting to csv\n",
    "\t# Removed transpose - check if actually required\n",
    "    clsf_report = pd.DataFrame(classification_report(y_true = true_labels, y_pred = predictions, output_dict=True, target_names = ['fake', 'real']))\n",
    "    clsf_report.to_csv(str('saved_models/'+model_name+'.csv'), index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the testset\n",
      "text_only_bert Text Only BERT Classification accuracy is\n",
      "81.64825828377231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.77      0.84      0.80      1031\n",
      "        real       0.86      0.80      0.83      1323\n",
      "\n",
      "    accuracy                           0.82      2354\n",
      "   macro avg       0.81      0.82      0.82      2354\n",
      "weighted avg       0.82      0.82      0.82      2354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'text_only_bert'\n",
    "path_to_model = 'saved_models/class_contrast_visualbert_lr_3e-05_val_loss_0.35285_ep_100.pt'\n",
    "evaluate(test_dataloader, model, model_name, path_to_model = path_to_model, load = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "112ac2046fa01f929d29fd747457323c8422411859b530c12136f6127f223c24"
  },
  "kernelspec": {
   "display_name": "Python 3.6.11 64-bit ('newnisbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
