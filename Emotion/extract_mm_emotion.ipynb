{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import asarray,zeros\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import timm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    os.environ['CUDA_ENVIRONMENT_DEVICES'] = \"0\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device\", device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Loading original data\n",
    "img_data = np.load(\"../data/image_array.npy\")\n",
    "labels_data = np.load(\"../data/labels.npy\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "num_images, sources, width, height, num_channels = img_data.shape\n",
    "img_data_reshape = np.reshape(img_data, newshape=(num_images, sources, num_channels, width, height))\n",
    "img_data_target = torch.tensor(img_data_reshape[:,0,:,:,:])\n",
    "img_data_source = torch.tensor(img_data_reshape[:,1,:,:,:]) \n",
    "print('New Target Shape', img_data_target.shape)\n",
    "print('New Source Shape', img_data_source.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New Target Shape torch.Size([11766, 3, 224, 224])\n",
      "New Source Shape torch.Size([11766, 3, 224, 224])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#TODO: Add Pytorch DataLoader\n",
    "def get_data_loader(batch_size, data, labels, split_type='train'):\n",
    "\tdata = TensorDataset(data, labels)\n",
    "\tif split_type == 'train':\n",
    "\t\tsampler = RandomSampler(data)\n",
    "\t\tdataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\telif split_type == 'val':\n",
    "\t\tsampler = SequentialSampler(data)\n",
    "\t\tdataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\treturn data, sampler, dataloader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "batch_size = 128\n",
    "test_data, test_sampler, test_dataloader = get_data_loader(batch_size, img_data_source, torch.tensor(labels_data, dtype=torch.long), 'val')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Import the main model\n",
    "#TODO: Define Resent-50 model\n",
    "class ResNetBottom(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ResNetBottom, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetBased, self).__init__()\n",
    "        self.vision_base_model = timm.create_model('resnet50', pretrained=True)\n",
    "        self.vision_model_head = ResNetBottom(self.vision_base_model)\n",
    "        self.project_1 = nn.Linear(2048, 1024, bias=True)\n",
    "        self.project_2 = nn.Linear(1024, 512, bias=True)\n",
    "        self.project_3 = nn.Linear(512, 128, bias=True)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.drop3 = nn.Dropout()\n",
    "        self.classification = nn.Linear(128, 6, bias=True)\n",
    "    def forward(self, img_features):\n",
    "        with torch.no_grad():\n",
    "            img_out = self.vision_model_head(img_features)\n",
    "        emotion_features = self.tanh3(self.project_3(self.tanh2(self.project_2(self.tanh1(self.project_1(img_out))))))\n",
    "        class_out = self.classification(emotion_features)\n",
    "        return emotion_features, class_out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Get the model\n",
    "emo_model = ResnetBased().to(device)\n",
    "emo_model.load_state_dict(torch.load('saved_models/emo_combine_res50_lr_3e-05_val_loss_0.60696_ep_98.pt'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "#TODO: Load 100d pre-trained Glove embeddings\n",
    "# Loading the pre-trained Glove embeddings\n",
    "embeddings_dict = {}\n",
    "with open(\"/sda/rina_1921cs13/Word_Embedding/glove/glove.6B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "#TODO: Construction scaffold labels\n",
    "emotion_real = embeddings_dict['sad']+embeddings_dict['joy']+embeddings_dict['love']\n",
    "emotion_fake = embeddings_dict['fear']+embeddings_dict['surprise']+embeddings_dict['anger']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "#TODO: Extract all emotion features\n",
    "all_emotion_features = []\n",
    "all_classes = []\n",
    "for i in range(0, len(img_data_source), 32):\n",
    "    print(i)\n",
    "    start_range = i\n",
    "    if start_range+32 >= len(img_data_source):\n",
    "        end_range = len(img_data_source)\n",
    "    else:\n",
    "        end_range = start_range + 32\n",
    "    emotion_features, class_out = emo_model(img_data_source[start_range:end_range, :, :, :].to(device))\n",
    "    class_out = np.argmax(class_out.cpu().detach().numpy(), axis=1)\n",
    "    all_emotion_features.extend(emotion_features.cpu().detach().numpy())\n",
    "    all_classes.extend(class_out)\n",
    "all_emotion_features = np.array(all_emotion_features)\n",
    "print(all_emotion_features.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "896\n",
      "928\n",
      "960\n",
      "992\n",
      "1024\n",
      "1056\n",
      "1088\n",
      "1120\n",
      "1152\n",
      "1184\n",
      "1216\n",
      "1248\n",
      "1280\n",
      "1312\n",
      "1344\n",
      "1376\n",
      "1408\n",
      "1440\n",
      "1472\n",
      "1504\n",
      "1536\n",
      "1568\n",
      "1600\n",
      "1632\n",
      "1664\n",
      "1696\n",
      "1728\n",
      "1760\n",
      "1792\n",
      "1824\n",
      "1856\n",
      "1888\n",
      "1920\n",
      "1952\n",
      "1984\n",
      "2016\n",
      "2048\n",
      "2080\n",
      "2112\n",
      "2144\n",
      "2176\n",
      "2208\n",
      "2240\n",
      "2272\n",
      "2304\n",
      "2336\n",
      "2368\n",
      "2400\n",
      "2432\n",
      "2464\n",
      "2496\n",
      "2528\n",
      "2560\n",
      "2592\n",
      "2624\n",
      "2656\n",
      "2688\n",
      "2720\n",
      "2752\n",
      "2784\n",
      "2816\n",
      "2848\n",
      "2880\n",
      "2912\n",
      "2944\n",
      "2976\n",
      "3008\n",
      "3040\n",
      "3072\n",
      "3104\n",
      "3136\n",
      "3168\n",
      "3200\n",
      "3232\n",
      "3264\n",
      "3296\n",
      "3328\n",
      "3360\n",
      "3392\n",
      "3424\n",
      "3456\n",
      "3488\n",
      "3520\n",
      "3552\n",
      "3584\n",
      "3616\n",
      "3648\n",
      "3680\n",
      "3712\n",
      "3744\n",
      "3776\n",
      "3808\n",
      "3840\n",
      "3872\n",
      "3904\n",
      "3936\n",
      "3968\n",
      "4000\n",
      "4032\n",
      "4064\n",
      "4096\n",
      "4128\n",
      "4160\n",
      "4192\n",
      "4224\n",
      "4256\n",
      "4288\n",
      "4320\n",
      "4352\n",
      "4384\n",
      "4416\n",
      "4448\n",
      "4480\n",
      "4512\n",
      "4544\n",
      "4576\n",
      "4608\n",
      "4640\n",
      "4672\n",
      "4704\n",
      "4736\n",
      "4768\n",
      "4800\n",
      "4832\n",
      "4864\n",
      "4896\n",
      "4928\n",
      "4960\n",
      "4992\n",
      "5024\n",
      "5056\n",
      "5088\n",
      "5120\n",
      "5152\n",
      "5184\n",
      "5216\n",
      "5248\n",
      "5280\n",
      "5312\n",
      "5344\n",
      "5376\n",
      "5408\n",
      "5440\n",
      "5472\n",
      "5504\n",
      "5536\n",
      "5568\n",
      "5600\n",
      "5632\n",
      "5664\n",
      "5696\n",
      "5728\n",
      "5760\n",
      "5792\n",
      "5824\n",
      "5856\n",
      "5888\n",
      "5920\n",
      "5952\n",
      "5984\n",
      "6016\n",
      "6048\n",
      "6080\n",
      "6112\n",
      "6144\n",
      "6176\n",
      "6208\n",
      "6240\n",
      "6272\n",
      "6304\n",
      "6336\n",
      "6368\n",
      "6400\n",
      "6432\n",
      "6464\n",
      "6496\n",
      "6528\n",
      "6560\n",
      "6592\n",
      "6624\n",
      "6656\n",
      "6688\n",
      "6720\n",
      "6752\n",
      "6784\n",
      "6816\n",
      "6848\n",
      "6880\n",
      "6912\n",
      "6944\n",
      "6976\n",
      "7008\n",
      "7040\n",
      "7072\n",
      "7104\n",
      "7136\n",
      "7168\n",
      "7200\n",
      "7232\n",
      "7264\n",
      "7296\n",
      "7328\n",
      "7360\n",
      "7392\n",
      "7424\n",
      "7456\n",
      "7488\n",
      "7520\n",
      "7552\n",
      "7584\n",
      "7616\n",
      "7648\n",
      "7680\n",
      "7712\n",
      "7744\n",
      "7776\n",
      "7808\n",
      "7840\n",
      "7872\n",
      "7904\n",
      "7936\n",
      "7968\n",
      "8000\n",
      "8032\n",
      "8064\n",
      "8096\n",
      "8128\n",
      "8160\n",
      "8192\n",
      "8224\n",
      "8256\n",
      "8288\n",
      "8320\n",
      "8352\n",
      "8384\n",
      "8416\n",
      "8448\n",
      "8480\n",
      "8512\n",
      "8544\n",
      "8576\n",
      "8608\n",
      "8640\n",
      "8672\n",
      "8704\n",
      "8736\n",
      "8768\n",
      "8800\n",
      "8832\n",
      "8864\n",
      "8896\n",
      "8928\n",
      "8960\n",
      "8992\n",
      "9024\n",
      "9056\n",
      "9088\n",
      "9120\n",
      "9152\n",
      "9184\n",
      "9216\n",
      "9248\n",
      "9280\n",
      "9312\n",
      "9344\n",
      "9376\n",
      "9408\n",
      "9440\n",
      "9472\n",
      "9504\n",
      "9536\n",
      "9568\n",
      "9600\n",
      "9632\n",
      "9664\n",
      "9696\n",
      "9728\n",
      "9760\n",
      "9792\n",
      "9824\n",
      "9856\n",
      "9888\n",
      "9920\n",
      "9952\n",
      "9984\n",
      "10016\n",
      "10048\n",
      "10080\n",
      "10112\n",
      "10144\n",
      "10176\n",
      "10208\n",
      "10240\n",
      "10272\n",
      "10304\n",
      "10336\n",
      "10368\n",
      "10400\n",
      "10432\n",
      "10464\n",
      "10496\n",
      "10528\n",
      "10560\n",
      "10592\n",
      "10624\n",
      "10656\n",
      "10688\n",
      "10720\n",
      "10752\n",
      "10784\n",
      "10816\n",
      "10848\n",
      "10880\n",
      "10912\n",
      "10944\n",
      "10976\n",
      "11008\n",
      "11040\n",
      "11072\n",
      "11104\n",
      "11136\n",
      "11168\n",
      "11200\n",
      "11232\n",
      "11264\n",
      "11296\n",
      "11328\n",
      "11360\n",
      "11392\n",
      "11424\n",
      "11456\n",
      "11488\n",
      "11520\n",
      "11552\n",
      "11584\n",
      "11616\n",
      "11648\n",
      "11680\n",
      "11712\n",
      "11744\n",
      "(11766, 128)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# Add bias to emotion features\n",
    "new_em_features = []\n",
    "for i, class_out in enumerate(all_classes):\n",
    "    if class_out == 1 and labels_data[i] == 1:\n",
    "        intermediate_features = np.concatenate((all_emotion_features[i], emotion_real))\n",
    "    elif class_out == 0:\n",
    "        intermediate_features = np.concatenate((all_emotion_features[i], emotion_fake))\n",
    "    else:\n",
    "        intermediate_features = np.concatenate((all_emotion_features[i], np.zeros(shape=(200))))\n",
    "    new_em_features.append(intermediate_features)\n",
    "new_em_arr = np.array(new_em_features)\n",
    "print('Bias emotion array shape', new_em_arr.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bias emotion array shape (11766, 328)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=128)\n",
    "new_em_arr_reduce = pca.fit_transform(new_em_arr)\n",
    "print('New emotion array shape', new_em_arr_reduce.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New emotion array shape (11766, 128)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "# Split to train and test\n",
    "# NOTE: Splitting data into train and test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(new_em_arr_reduce, labels_data, test_size=0.2, random_state=43)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "# Reshape Labels\n",
    "new_train_labels = np.reshape(train_labels, newshape=(train_labels.shape[0]))\n",
    "print(new_train_labels.shape)\n",
    "new_test_labels = np.reshape(test_labels, newshape=(test_labels.shape[0]))\n",
    "print(new_test_labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9412,)\n",
      "(2354,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "# TODO: Fit a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(max_iter=500)\n",
    "logisticRegr.fit(train_data, new_train_labels)\n",
    "predictions = logisticRegr.predict(test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "print('Unbiased Emotion Classification accuracy is')\n",
    "print(metrics.accuracy_score(test_labels, predictions)*100)\n",
    "print(classification_report(test_labels, predictions, target_names = ['fake', 'real']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unbiased Emotion Classification accuracy is\n",
      "98.68309260832625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       1.00      0.97      0.98      1031\n",
      "        real       0.98      1.00      0.99      1323\n",
      "\n",
      "    accuracy                           0.99      2354\n",
      "   macro avg       0.99      0.98      0.99      2354\n",
      "weighted avg       0.99      0.99      0.99      2354\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# Saving emotion representations\n",
    "np.save('../data/emotion_reprs', new_em_arr_reduce)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.11 64-bit ('newnisbert': conda)"
  },
  "interpreter": {
   "hash": "b4396559e1bf5c32d41ffba8c548c213f47855d2d4ac82024dddc5ba3ffea815"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}